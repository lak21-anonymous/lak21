{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /yw_data/robert_gold/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import nltk\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from nltk.collocations import *\n",
    "from nltk.metrics import TrigramAssocMeasures \n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from graphing import *\n",
    "from constants import *\n",
    "from utilities import *\n",
    "from preprocess import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_results = pickle.load(open('merged_results.pickle', 'rb'))\n",
    "merged_results['Valid Vector'] = merged_results['Skip Thought Vector'].apply(lambda x: ~np.isnan(x).any())\n",
    "removed_results = merged_results[~merged_results['Valid Vector']]\n",
    "merged_results = merged_results[merged_results['Valid Vector']]\n",
    "labeled_results = merged_results[merged_results['Categorical Tag'] != 'no tag']\n",
    "q1_results = merged_results[merged_results['Question'] == Q1]\n",
    "q1_labeled_results = labeled_results[labeled_results['Question'] == Q1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/yw_data/robert_gold/.local/lib/python3.6/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "q1_features = np.array(q1_labeled_results['Skip Thought Vector'].tolist())\n",
    "q1_labels = np.array(q1_labeled_results['Categorical Tag'])\n",
    "clf = SVC()\n",
    "clf.fit(q1_features, q1_labels)\n",
    "predictions = clf.predict(list(q1_results['Skip Thought Vector']))\n",
    "q1_results['sentiment'] = predictions\n",
    "q2_results = merged_results[merged_results['Question'] == Q2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_common_phrases(results, use_trigrams = True, freq_filter = 3, result_count = 10, ignore_stops = False, window_size = 0):\n",
    "    responses = list(results['Answer'].apply(lambda x: x.split()))\n",
    "    words = [w for tokens in responses for w in tokens + ['BREAK']]\n",
    "    measures = TrigramAssocMeasures() if use_trigrams else nltk.collocations.BigramAssocMeasures()\n",
    "    f = TrigramCollocationFinder if use_trigrams else BigramCollocationFinder\n",
    "    finder = f.from_words(words, window_size = window_size) if window_size > 2 else f.from_words(words)\n",
    "    #finder = TrigramCollocationFinder.from_words(words) if use_trigrams else BigramCollocationFinder.from_words(words)\n",
    "    if ignore_stops:\n",
    "        ignored_words = nltk.corpus.stopwords.words('english')\n",
    "        filter_stops = lambda w: len(w) < 2 or w in ignored_words \n",
    "        filter_ngram_stops = lambda *ngram: len([w for w in ngram if w not in ignored_words and len(w) > 2]) < 2\n",
    "        # finder.apply_word_filter(filter_stops)\n",
    "        finder.apply_ngram_filter(filter_ngram_stops) \n",
    "    response_count_filter = lambda *ngram: results[results['Answer'].str.contains(' '.join(ngram))]['Answer'].shape[0] < freq_filter\n",
    "    print_filter = lambda *ngram: print(results[results['Answer'].str.contains(' '.join(ngram))]['Answer'].shape[0])\n",
    "    finder.apply_ngram_filter(response_count_filter)\n",
    "    finder.apply_word_filter(lambda w: w == 'BREAK')\n",
    "    finder.apply_freq_filter(freq_filter)\n",
    "    return finder.nbest(measures.pmi, result_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anal zed& 3\\\\\n",
      "eric sir& 3\\\\\n",
      "square root& 6\\\\\n",
      "stack overflow& 5\\\\\n",
      "truth table& 6\\\\\n",
      "glass box& 10\\\\\n",
      "pay attention& 6\\\\\n",
      "critical thinking& 3\\\\\n",
      "straight forward& 14\\\\\n",
      "close enough& 4\\\\\n"
     ]
    }
   ],
   "source": [
    "phrases = get_common_phrases(merged_results[merged_results['Question'] == Q2], use_trigrams = False, ignore_stops = True)\n",
    "phrase_strings = [' '.join(phrase) for phrase in phrases]\n",
    "for phrase_string in phrase_strings:\n",
    "    print(f\"{phrase_string}& {merged_results[merged_results['Answer'].str.contains(phrase_string)]['Answer'].shape[0]}\\\\\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMI Rank&fex1&fex2&fex4&ps1&ps2&ps4\\\\\n",
      "1&leave to right&step by step&glass box test&line by line&into small piece&piece by piece\\\\\n",
      "2&evaluate each part&follow the instruction&use python tutor&problem into small&an infinite loop&play game function\\\\\n",
      "3&trial and error&from previous video&the discussion thread&problem solve process&use python tutor&this problem set\\\\\n"
     ]
    }
   ],
   "source": [
    "def print_latex_table(count):\n",
    "    print('PMI Rank&', end = '')\n",
    "    for problem in PROBLEMS:\n",
    "        end_char = \"\\\\\\\\\\n\" if problem == 'ps4' else '&'\n",
    "        print(problem, end =end_char)\n",
    "\n",
    "    for k in range(count):\n",
    "        print(k+1, end = \"&\")\n",
    "        for problem in PROBLEMS:\n",
    "            phrases = get_common_phrases(q2_results[q2_results['Problem'] == problem], use_trigrams = True, ignore_stops = True, freq_filter = 3, window_size = 3, result_count = count)\n",
    "            end_char = \"\\\\\\\\\\n\" if problem == 'ps4' else '&'\n",
    "            print(' '.join(phrases[k]), end = end_char)\n",
    "\n",
    "print_latex_table(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fex1\n",
      "1. leave to right\n",
      "2. evaluate each part\n",
      "3. trial and error\n",
      "\n",
      "\n",
      "fex2\n",
      "1. step by step\n",
      "2. follow the instruction\n",
      "3. from previous video\n",
      "\n",
      "\n",
      "fex4\n",
      "1. glass box test\n",
      "2. use python tutor\n",
      "3. the discussion thread\n",
      "\n",
      "\n",
      "ps1\n",
      "1. line by line\n",
      "2. problem into small\n",
      "3. problem solve process\n",
      "\n",
      "\n",
      "ps2\n",
      "1. into small piece\n",
      "2. an infinite loop\n",
      "3. use python tutor\n",
      "\n",
      "\n",
      "ps4\n",
      "1. piece by piece\n",
      "2. play game function\n",
      "3. this problem set\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for problem in PROBLEMS:\n",
    "    phrases = get_common_phrases(q2_results[q2_results['Problem'] == problem], use_trigrams = True, ignore_stops = True, freq_filter = 3, window_size = 3, result_count = 3)\n",
    "    print(problem)\n",
    "    [print(f\"{k+1}. {' '.join(phrases[k])}\") for k in range(len(phrases))]\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n",
      "1. look forward\n",
      "2. little bit\n",
      "3. well prepared\n",
      "4. problem set\n",
      "5. feel like\n",
      "6. pretty good\n",
      "7. nice exercise\n",
      "8. good job\n",
      "9. good work\n",
      "10. interesting exercise\n",
      "\n",
      "\n",
      "neutral\n",
      "1. right answer\n",
      "2. additional feedback\n",
      "\n",
      "\n",
      "negative\n",
      "1. discussion thread\n",
      "2. similarity checker\n",
      "3. secret number\n",
      "4. even though\n",
      "5. click submit\n",
      "6. prior knowledge\n",
      "7. helper function\n",
      "8. discussion section\n",
      "9. print statement\n",
      "10. test case\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sentiment in SENTIMENTS:\n",
    "    phrases = get_common_phrases(q1_results[q1_results['sentiment'] == sentiment], use_trigrams = False, ignore_stops = True, freq_filter = 3, window_size = 0)\n",
    "    print(sentiment)\n",
    "    [print(f\"{k+1}. {' '.join(phrases[k])}\") for k in range(len(phrases))]\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3614\n",
      "542\n",
      "540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/yw_data/robert_gold/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/yw_data/robert_gold/.local/lib/python3.6/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107\n",
      "107\n",
      "433\n"
     ]
    }
   ],
   "source": [
    "def preprocess_results(results):\n",
    "    print(results.shape[0])\n",
    "    results['Answer'] = results['Answer'].str.strip()\n",
    "    results = results[~results['Answer'].isin(EXCLUDED_ANSWERS)]\n",
    "    print(results.shape[0])\n",
    "    results['english'] = results['Answer'].apply(is_english)\n",
    "    results = results[results['english']]\n",
    "    print(results.shape[0])\n",
    "    results['Original'] = results['Answer']\n",
    "    results['Answer'] = results['Answer'].apply(normalize)\n",
    "    \n",
    "    results['nonsense'] = results['Answer'].apply(is_nonsense)\n",
    "    nonsense = results[results['nonsense']]\n",
    "    print(nonsense.shape[0])\n",
    "    print(nonsense[nonsense['Original'].str.split().str.len()<2].shape[0])\n",
    "    results = results[~results['nonsense']]\n",
    "    results['Manual Tag'] = 'no tag'\n",
    "    print(results.shape[0])\n",
    "    return results\n",
    "\n",
    "\n",
    "unprocessed_results = get_problem_data(DATA)\n",
    "complete_results = merge_problem_data(unprocessed_results)\n",
    "extra_results = complete_results[complete_results['Question'] == extra_question]\n",
    "extra_results = preprocess_results(extra_results)\n",
    "# extra_results = extra_results[~(extra_results['Answer'] == 'Unanswered')]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. git hub\n",
      "2. stack overflow\n",
      "3. function call\n",
      "4. pythontutor com\n",
      "5. bisection search\n",
      "6. problem set\n",
      "7. recommend textbook\n",
      "8. text book\n",
      "9. google search\n",
      "10. program use\n",
      "11. python tutor\n",
      "12. learn python\n",
      "13. previous python\n",
      "14. python knowledge\n",
      "15. use python\n",
      "16. course textbook\n",
      "17. course book\n",
      "18. python book\n",
      "19. python course\n",
      "1&git hub\\\\\n",
      "2&stack overflow\\\\\n",
      "3&function call\\\\\n",
      "4&pythontutor com\\\\\n",
      "5&bisection search\\\\\n",
      "6&problem set\\\\\n",
      "7&recommend textbook\\\\\n",
      "8&text book\\\\\n",
      "9&google search\\\\\n",
      "10&program use\\\\\n",
      "11&python tutor\\\\\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "phrases = get_common_phrases(extra_results, use_trigrams = False, ignore_stops = True, freq_filter = 3, window_size = 0, result_count = 20)\n",
    "phrase_strings = [' '.join(phrase) for phrase in phrases]\n",
    "[print(f\"{k+1}. {phrase_strings[k]}\") for k in range(len(phrases))]\n",
    "[print(f\"{k+1}&{phrase_strings[k]}\\\\\\\\\") for k in range(11)]\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "git hub: 3\n",
      "stack overflow: 8\n",
      "function call: 3\n",
      "pythontutor com: 6\n",
      "bisection search: 4\n",
      "problem set: 3\n",
      "recommend textbook: 3\n",
      "text book: 3\n",
      "google search: 5\n",
      "program use: 4\n",
      "python tutor: 13\n",
      "learn python: 3\n",
      "previous python: 4\n",
      "python knowledge: 4\n",
      "use python: 6\n",
      "course textbook: 3\n",
      "course book: 3\n",
      "python book: 3\n",
      "python course: 3\n"
     ]
    }
   ],
   "source": [
    "for phrase_string in phrase_strings:\n",
    "    print(f\"{phrase_string}: {extra_results[extra_results['Answer'].str.contains(phrase_string)]['Answer'].shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 10\n",
    "reduced_q2_encodings = reduce_dimensions(list(q2_results['Skip Thought Vector']))\n",
    "agg_labels, model = get_agg_clusters(list(q2_results['Skip Thought Vector']), n_clusters = n_clusters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/yw_data/robert_gold/.local/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. pretty straight forward\n",
      "2. glass box test\n",
      "3. into small piece\n",
      "\n",
      "\n",
      "1. step by step\n",
      "2. follow the instruction\n",
      "3. read the instruction\n",
      "\n",
      "\n",
      "1. divide and conquer\n",
      "2. trial and error\n",
      "3. line by line\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1. use python tutor\n",
      "2. piece by piece\n",
      "3. step by step\n",
      "\n",
      "\n",
      "1. step by step\n",
      "2. line of code\n",
      "3. type the code\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1. trail and error\n",
      "2. trial and error\n",
      "3. try and error\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q2_results['Agg Label'] = agg_labels\n",
    "for k in range (n_clusters):\n",
    "    phrases = get_common_phrases(q2_results[q2_results['Agg Label'] == k], use_trigrams = True, ignore_stops = True, freq_filter = 3, window_size = 3, result_count = 3)\n",
    "    [print(f\"{k+1}. {' '.join(phrases[k])}\") for k in range(len(phrases))]\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
