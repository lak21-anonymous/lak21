{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a6830e4af546>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanifold\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTSNE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAgglomerativeClustering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import nltk\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from nltk.collocations import *\n",
    "from nltk.metrics import TrigramAssocMeasures \n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from graphing import *\n",
    "from constants import *\n",
    "from utilities import *\n",
    "from preprocess import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Preliminary stuff, load data and calculate sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_results = pickle.load(open('merged_results.pickle', 'rb'))\n",
    "merged_results['Valid Vector'] = merged_results['Skip Thought Vector'].apply(lambda x: ~np.isnan(x).any())\n",
    "removed_results = merged_results[~merged_results['Valid Vector']]\n",
    "merged_results = merged_results[merged_results['Valid Vector']]\n",
    "labeled_results = merged_results[merged_results['Categorical Tag'] != 'no tag']\n",
    "q1_results = merged_results[merged_results['Question'] == Q1]\n",
    "q1_labeled_results = labeled_results[labeled_results['Question'] == Q1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_features = np.array(q1_labeled_results['Skip Thought Vector'].tolist())\n",
    "q1_labels = np.array(q1_labeled_results['Categorical Tag'])\n",
    "clf = SVC()\n",
    "clf.fit(q1_features, q1_labels)\n",
    "predictions = clf.predict(list(q1_results['Skip Thought Vector']))\n",
    "q1_results['sentiment'] = predictions\n",
    "q2_results = merged_results[merged_results['Question'] == Q2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Find common phrases in the responses\n",
    "    - results: data to be used, should be pandas Series\n",
    "    - use_trigrams: if true, find trigram, if false, find bigrams\n",
    "    - freq_filter: only includes responses if they appear at least this many times in the text\n",
    "    - result_count: the number of phrases to return\n",
    "    - ignore_stops: if true, filter out any n-grams which contain more than 2 stop words\n",
    "    - window_size: How close toegther the n-grams should be, if 0 then the phrases are consecutive words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_common_phrases(results, use_trigrams = True, freq_filter = 3, result_count = 10, ignore_stops = False, window_size = 0):\n",
    "    responses = list(results['Answer'].apply(lambda x: x.split()))\n",
    "    words = [w for tokens in responses for w in tokens + ['BREAK']]\n",
    "    measures = TrigramAssocMeasures() if use_trigrams else nltk.collocations.BigramAssocMeasures()\n",
    "    f = TrigramCollocationFinder if use_trigrams else BigramCollocationFinder\n",
    "    finder = f.from_words(words, window_size = window_size) if window_size > 2 else f.from_words(words)\n",
    "    #finder = TrigramCollocationFinder.from_words(words) if use_trigrams else BigramCollocationFinder.from_words(words)\n",
    "    if ignore_stops:\n",
    "        ignored_words = nltk.corpus.stopwords.words('english')\n",
    "        filter_stops = lambda w: len(w) < 2 or w in ignored_words \n",
    "        filter_ngram_stops = lambda *ngram: len([w for w in ngram if w not in ignored_words and len(w) > 2]) < 2\n",
    "        # finder.apply_word_filter(filter_stops)\n",
    "        finder.apply_ngram_filter(filter_ngram_stops) \n",
    "    response_count_filter = lambda *ngram: results[results['Answer'].str.contains(' '.join(ngram))]['Answer'].shape[0] < freq_filter\n",
    "    print_filter = lambda *ngram: print(results[results['Answer'].str.contains(' '.join(ngram))]['Answer'].shape[0])\n",
    "    finder.apply_ngram_filter(response_count_filter)\n",
    "    finder.apply_word_filter(lambda w: w == 'BREAK')\n",
    "    finder.apply_freq_filter(freq_filter)\n",
    "    return finder.nbest(measures.pmi, result_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- as written, finds the most common bigrams by PMI in the question two responses and prints them in a LaTex friendly way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = get_common_phrases(merged_results[merged_results['Question'] == Q2], use_trigrams = False, ignore_stops = True)\n",
    "phrase_strings = [' '.join(phrase) for phrase in phrases]\n",
    "for phrase_string in phrase_strings:\n",
    "    print(f\"{phrase_string}& {merged_results[merged_results['Answer'].str.contains(phrase_string)]['Answer'].shape[0]}\\\\\\\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- finds the 3 most common trigrams in each exercises responses and prints them in a LaTex friendly way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_latex_table(count):\n",
    "    print('PMI Rank&', end = '')\n",
    "    for problem in PROBLEMS:\n",
    "        end_char = \"\\\\\\\\\\n\" if problem == 'ps4' else '&'\n",
    "        print(problem, end =end_char)\n",
    "\n",
    "    for k in range(count):\n",
    "        print(k+1, end = \"&\")\n",
    "        for problem in PROBLEMS:\n",
    "            phrases = get_common_phrases(q2_results[q2_results['Problem'] == problem], use_trigrams = True, ignore_stops = True, freq_filter = 3, window_size = 3, result_count = count)\n",
    "            end_char = \"\\\\\\\\\\n\" if problem == 'ps4' else '&'\n",
    "            print(' '.join(phrases[k]), end = end_char)\n",
    "\n",
    "print_latex_table(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- same as above but prints in a less LaTex friendly way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for problem in PROBLEMS:\n",
    "    phrases = get_common_phrases(q2_results[q2_results['Problem'] == problem], use_trigrams = True, ignore_stops = True, freq_filter = 3, window_size = 3, result_count = 3)\n",
    "    print(problem)\n",
    "    [print(f\"{k+1}. {' '.join(phrases[k])}\") for k in range(len(phrases))]\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_results(results):\n",
    "    print(results.shape[0])\n",
    "    results['Answer'] = results['Answer'].str.strip()\n",
    "    results = results[~results['Answer'].isin(EXCLUDED_ANSWERS)]\n",
    "    print(results.shape[0])\n",
    "    results['english'] = results['Answer'].apply(is_english)\n",
    "    results = results[results['english']]\n",
    "    print(results.shape[0])\n",
    "    results['Original'] = results['Answer']\n",
    "    results['Answer'] = results['Answer'].apply(normalize)\n",
    "    \n",
    "    results['nonsense'] = results['Answer'].apply(is_nonsense)\n",
    "    nonsense = results[results['nonsense']]\n",
    "    print(nonsense.shape[0])\n",
    "    print(nonsense[nonsense['Original'].str.split().str.len()<2].shape[0])\n",
    "    results = results[~results['nonsense']]\n",
    "    results['Manual Tag'] = 'no tag'\n",
    "    print(results.shape[0])\n",
    "    return results\n",
    "\n",
    "\n",
    "unprocessed_results = get_problem_data(DATA)\n",
    "complete_results = merge_problem_data(unprocessed_results)\n",
    "extra_results = complete_results[complete_results['Question'] == extra_question]\n",
    "extra_results = preprocess_results(extra_results)\n",
    "# extra_results = extra_results[~(extra_results['Answer'] == 'Unanswered')]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- prints the highest ranking bigrams in response to question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = get_common_phrases(extra_results, use_trigrams = False, ignore_stops = True, freq_filter = 3, window_size = 0, result_count = 20)\n",
    "phrase_strings = [' '.join(phrase) for phrase in phrases]\n",
    "[print(f\"{k+1}. {phrase_strings[k]}\") for k in range(len(phrases))]\n",
    "[print(f\"{k+1}&{phrase_strings[k]}\\\\\\\\\") for k in range(11)]\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
