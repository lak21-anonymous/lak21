{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn import svm\n",
    "\n",
    "from supervised_sentiment_analysis import *\n",
    "from constants import *\n",
    "from utilities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_results = pickle.load(open('merged_results.P', 'rb'))\n",
    "merged_results['Valid Vector'] = merged_results['Skip Thought Vector'].apply(lambda x: ~np.isnan(x).any())\n",
    "removed_results = merged_results[~merged_results['Valid Vector']]\n",
    "merged_results = merged_results[merged_results['Valid Vector']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_results = merged_results[merged_results['Categorical Tag'] != 'no tag']\n",
    "q1_results = merged_results[merged_results['Question'] == Q1]\n",
    "q1_labeled_results = labeled_results[labeled_results['Question'] == Q1]\n",
    "q1_features = np.array(q1_labeled_results['Skip Thought Vector'].tolist())\n",
    "q1_labels = np.array(q1_labeled_results['Categorical Tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC()\n",
    "clf.fit(q1_features, q1_labels)\n",
    "predictions = clf.predict(list(q1_results['Skip Thought Vector']))\n",
    "q1_results['prediction'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_sentiment(results):\n",
    "    y_pred = list(results['prediction'])\n",
    "    total = max(len(y_pred), 1)\n",
    "    positive_count = len([prediction for prediction in y_pred if prediction == 'positive'])\n",
    "    positive_score = positive_count/total\n",
    "    negative_count = len([prediction for prediction in y_pred if prediction == 'negative'])\n",
    "    negative_score = negative_count/total\n",
    "    \n",
    "    return [positive_score, negative_score, 1-positive_score-negative_score, positive_score-negative_score]\n",
    "get_average_sentiment(q1_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_scores(results):\n",
    "    problem_keys = {\n",
    "        'fex1': 0,\n",
    "        'fex2': 1,\n",
    "        'fex4': 2,\n",
    "        'ps1': 3,\n",
    "        'ps2': 4,\n",
    "        'ps4': 5,\n",
    "    }\n",
    "    results['problem key'] = results['Problem'].apply(lambda x: problem_keys[x])\n",
    "    subsets = [[k] for k in range(6)]+[[0,3],[1,4],[2,5],[0,1,2],[3,4,5], list(range(6))]\n",
    "    sentiment_scores = []\n",
    "    for subset in subsets:\n",
    "        subset_results = results[results['problem key'].isin(subset)]\n",
    "        sentiment_scores.append(get_average_sentiment(subset_results))\n",
    "\n",
    "    arr = np.array(sentiment_scores)\n",
    "    \n",
    "    index = ['fex1', 'fex2', 'fex4', 'ps1', 'ps2', 'ps4', '1', '2', '4', 'fex', 'ps', 'total']\n",
    "    columns = ['Positive', 'Negative', 'Neutral', 'Sentiment']\n",
    "    rounded = np.round(arr, 2)\n",
    "    return pd.DataFrame(rounded, columns=columns, index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment_scores(q1_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_responses = q1_results[q1_results['prediction'] == 'positive']['Answer'].str.split().str.len()\n",
    "negative_responses = q1_results[q1_results['prediction'] == 'negative']['Answer'].str.split().str.len()\n",
    "neutral_responses = q1_results[q1_results['prediction'] == 'neutral']['Answer'].str.split().str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lengths(results, stacked = False):\n",
    "    CUTOFF = 20\n",
    "    plt.figure(figsize=(10,10))\n",
    "    bins = np.arange(0, CUTOFF, 1)\n",
    "    labels = ['positive', 'negative', 'neutral']\n",
    "    arr = plt.hist(results, \n",
    "                   stacked = stacked, \n",
    "                   bins = bins, \n",
    "                   alpha = 0.8, \n",
    "                   label = labels)\n",
    "    plt.legend(prop={'size': 16})\n",
    "    plt.grid(axis= 'y', alpha= 0.75)\n",
    "    plt.xlabel('Number of Words', size = 18)\n",
    "    plt.xticks(np.arange(0, CUTOFF+1, 2), size = 16)\n",
    "    plt.yticks(size = 16)\n",
    "    plt.ylabel('Number of Responses', size = 18)\n",
    "    plt.title('Distribution of Word Count per Response', size = 18)\n",
    "    # set up counts above each bar\n",
    "    bin_width = arr[1][1]-arr[1][0]\n",
    "\n",
    "plot_lengths([positive_responses, negative_responses, neutral_responses])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_count_profile(username):\n",
    "    results = q1_results[q1_results['username'] == username]\n",
    "    return [len(results), len(results['prediction'].unique()), len(results[results['prediction'] == 'negative'])]\n",
    "\n",
    "q1_results['answer count profile'] = q1_results['username'].apply(answer_count_profile)\n",
    "q1_results['answer count'] = q1_results['answer count profile'].apply(lambda x: x[0])\n",
    "q1_results['unique count'] = q1_results['answer count profile'].apply(lambda x: x[1])\n",
    "q1_results['negative count'] = q1_results['answer count profile'].apply(lambda x: x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swing_voter_results = q1_results[q1_results['unique count'] > 1]\n",
    "len(swing_voter_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(q1_results[(q1_results['unique count'] == 1) & (q1_results['answer count'] > 1)]['username'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(q1_results[q1_results['answer count'] == 1]['username'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment_scores(swing_voter_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_results = q1_results[(q1_results['negative count'] > 0) & (q1_results['answer count'] > 1)]\n",
    "len(negative_results), len(negative_results['username'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment_scores(negative_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unprocessed_results = get_problem_data(DATA)\n",
    "results = merge_problem_data(unprocessed_results)\n",
    "results['username'].unique().shape, results.columns, results['Question'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sections = {\n",
    "    1: results[(results['Problem'] == 'fex1') | (results['Problem'] == 'ps1')],\n",
    "    2: results[(results['Problem'] == 'fex2') | (results['Problem'] == 'ps2')],\n",
    "    4: results[(results['Problem'] == 'fex4') | (results['Problem'] == 'ps4')]\n",
    "}\n",
    "def completed_x(username, section = 4):\n",
    "    results_x = sections[section]\n",
    "    return results_x[results_x['username'] == username].shape[0] > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[((results['Problem'] == 'fex4') | (results['Problem'] == 'ps4'))]['username'].unique().shape\n",
    "usernames = results['username'].unique()\n",
    "usernames = pd.DataFrame(usernames, columns = ['username'])\n",
    "usernames['completed_4'] = usernames['username'].apply(completed_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_results['completed_4'] = q1_results['username'].apply(completed_x)\n",
    "q1_results['completed_1'] = q1_results['username'].apply(lambda x: completed_x(x, section = 1))\n",
    "q1_results['completed_2'] = q1_results['username'].apply(lambda x: completed_x(x, section = 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Want to find some way to classify students bassd on if they completed the course or not\n",
    "- expectation is students who dropped out are more negative\n",
    "- Students who responded to a question related to fex4 or ps4 probably did\n",
    "- Students who did not may or may not have\n",
    "- table represents difference in sentiment between two groups\n",
    "- students who did respond to a section 4 problem (more likely to complete course) are surprisingly more negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_differences = (get_sentiment_scores(q1_results[q1_results['completed_4']])-get_sentiment_scores(q1_results[~q1_results['completed_4']]))\n",
    "score_differences.loc[['fex1', 'fex2', 'ps1', 'ps2']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This shows that everyone is asked at least one question in response to question 1, so I think it means everyone is asked at least one question in response to every problem?\n",
    "- Anyway, this shows a sort of metric on dropout rates, 48 students drop out out of the 1937 from section 1 to section 2\n",
    "- A lot more drop out on the way to section 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_results[q1_results['completed_4']].shape, q1_results[~q1_results['completed_4']].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_results[q1_results['completed_1']].shape, q1_results[~q1_results['completed_1']].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_results[q1_results['completed_2']].shape, q1_results[~q1_results['completed_2']].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
