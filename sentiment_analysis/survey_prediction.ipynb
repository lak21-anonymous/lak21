{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'plot_confusion_matrix'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a9df35e96710>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m  \u001b[0;31m# doctest: +SKIP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmake_classification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_confusion_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSVC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'plot_confusion_matrix'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt  # doctest: +SKIP\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from scipy.stats import kruskal\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.stats import kendalltau\n",
    "\n",
    "from supervised_sentiment_analysis import *\n",
    "from constants import *\n",
    "from utilities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_results = pickle.load(open('merged_results.P', 'rb'))\n",
    "merged_results['Valid Vector'] = merged_results['Skip Thought Vector'].apply(lambda x: ~np.isnan(x).any())\n",
    "removed_results = merged_results[~merged_results['Valid Vector']]\n",
    "merged_results = merged_results[merged_results['Valid Vector']]\n",
    "labeled_results = merged_results[merged_results['Categorical Tag'] != 'no tag']\n",
    "q1_results = merged_results[merged_results['Question'] == Q1]\n",
    "q1_labeled_results = labeled_results[labeled_results['Question'] == Q1]\n",
    "q1_features = np.array(q1_labeled_results['Skip Thought Vector'].tolist())\n",
    "q1_labels = np.array(q1_labeled_results['Categorical Tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unprocessed_results = get_problem_data(DATA)\n",
    "complete_results = merge_problem_data(unprocessed_results)\n",
    "cont_results = complete_results[complete_results['Question'] == cont_question]\n",
    "challenge_results = complete_results[complete_results['Question'] == challenge_question]\n",
    "prepared_results = complete_results[complete_results['Question'] == prepared_question]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total number of students: {complete_results['username'].unique().shape[0]} \\n \\\n",
    "      Total number of responses {complete_results.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_continuity_prediction(row):\n",
    "    username = row['username']\n",
    "    problem = row['Problem']\n",
    "    cont_prediction = cont_results[(cont_results['username'] == username) & (cont_results['Problem'] == problem)]['Answer']\n",
    "    assert len(cont_prediction) < 2\n",
    "    if len(cont_prediction) == 0:\n",
    "        return 'none'\n",
    "    return cont_prediction.iloc[0]\n",
    "merged_results['continuity'] = merged_results.apply(get_continuity_prediction, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_challenge(row):\n",
    "    username = row['username']\n",
    "    problem = row['Problem']\n",
    "    challenge_prediction = challenge_results[(challenge_results['username'] == username) & (challenge_results['Problem'] == problem)]['Answer']\n",
    "    assert len(challenge_prediction) < 2\n",
    "    if len(challenge_prediction) == 0:\n",
    "        return 'none'\n",
    "    return challenge_prediction.iloc[0]\n",
    "merged_results['challenge'] = merged_results.apply(get_challenge, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prepared(row):\n",
    "    username = row['username']\n",
    "    problem = row['Problem']\n",
    "    prediction = prepared_results[(prepared_results['username'] == username) & (prepared_results['Problem'] == problem)]['Answer']\n",
    "    assert len(prediction) < 2\n",
    "    if len(prediction) == 0:\n",
    "        return 'none'\n",
    "    return prediction.iloc[0]\n",
    "merged_results['prepared'] = merged_results.apply(get_prepared, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "q1_results = merged_results[merged_results['Question'] == Q1]\n",
    "clf = SVC()\n",
    "clf.fit(q1_features, q1_labels)\n",
    "predictions = clf.predict(list(q1_results['Skip Thought Vector']))\n",
    "q1_results['prediction'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge_responses = ['Not at all challenging', 'Slightly challenging', \n",
    "                       'Moderately challenging', 'Very challenging', \n",
    "                       'Extremely challenging/']\n",
    "continuity_responses = ['Not at all', 'Slightly', 'Moderately', 'Very', 'Extremely']\n",
    "prepared_responses = ['Not at all prepared', 'Slightly prepared', 'Moderately prepared', 'Very prepared', 'Extremely prepared']\n",
    "problems = ['fex1', 'fex2', 'fex4', 'ps1', 'ps2', 'ps4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_sentiment(results):\n",
    "    y_pred = list(results['prediction'])\n",
    "    total = max(len(y_pred), 1)\n",
    "    positive_count = len([prediction for prediction in y_pred if prediction == 'positive'])\n",
    "    positive_score = positive_count/total\n",
    "    negative_count = len([prediction for prediction in y_pred if prediction == 'negative'])\n",
    "    negative_score = negative_count/total\n",
    "    \n",
    "    return [positive_score, negative_score, 1-positive_score-negative_score, positive_score-negative_score]\n",
    "\n",
    "def plot_sentiment_distribution(results, key = 'challenge', labels = challenge_responses):\n",
    "    sentiment_scores = []\n",
    "    response_counts = []\n",
    "    for label in labels:\n",
    "        label_results = results[results[key] == label]\n",
    "        sentiment_scores.append(get_average_sentiment(label_results))\n",
    "        response_counts.append(len(label_results))\n",
    "    ind = range(len(sentiment_scores))\n",
    "    fig, ax = plt.subplots(figsize = (8,8))\n",
    "    positive_scores = np.array([score[0] for score in sentiment_scores])\n",
    "    negative_scores = np.array([score[1] for score in sentiment_scores])\n",
    "    neutral_scores = np.array([score[2] for score in sentiment_scores])\n",
    "    short_labels = [f\"{labels[k]}:\\n{response_counts[k]}\" for k in range(len(labels))]\n",
    "    if (labels == challenge_responses) or (labels == prepared_responses):\n",
    "        short_labels = [f\"{' '.join(labels[k].split()[:-1])}:\\n{response_counts[k]}\" for k in range(len(labels))]\n",
    "    negative_bars = ax.bar(short_labels, negative_scores, label = 'negative')\n",
    "    neutral_bars = ax.bar(short_labels, neutral_scores, label = 'neutral', bottom = negative_scores)\n",
    "    positive_bars = ax.bar(short_labels, positive_scores, label = 'negative', bottom = negative_scores + neutral_scores)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "    ax.set_title(\"Proportion of sentiment classifications by student's\\nQuestion 5 (preparedness) response\", size = 16)\n",
    "    ax.set_xlabel(\"Response: Response Count\", size = 16)\n",
    "    plt.legend((negative_bars, neutral_bars, positive_bars), ('Negative', 'Neutral', 'Positive'), prop={'size': 16})\n",
    "    plt.show()\n",
    "    return sentiment_scores\n",
    "\n",
    "def plot_sentiment_scores(results, key = 'challenge', labels = challenge_responses):\n",
    "    sentiment_scores = []\n",
    "    response_counts = []\n",
    "    for label in labels:\n",
    "        label_results = results[results[key] == label]\n",
    "        sentiment_scores.append(get_average_sentiment(label_results)[3])\n",
    "        response_counts.append(len(label_results))\n",
    "    ind = range(len(sentiment_scores))\n",
    "    fig, ax = plt.subplots(figsize = (10,10))\n",
    "    short_labels = [f\"{labels[k]}: {response_counts[k]}\" for k in range(len(labels))]\n",
    "    if (labels == challenge_responses) or (labels == prepared_responses):\n",
    "        short_labels = [f\"{' '.join(labels[k].split()[:-1])}: {response_counts[k]}\" for k in range(len(labels))]\n",
    "    sentiment_bars = ax.scatter(short_labels, sentiment_scores, label = 'sentiment')\n",
    "    ax.set_title(f\"Sentiment score by student's {key} response\")\n",
    "    ax.set_xlabel(\"Response: Response Count\")\n",
    "    plt.show()\n",
    "    return sentiment_scores\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sentiment_distribution(q1_results[q1_results['prepared'] != 'none'], key = 'prepared', labels = prepared_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sentiment_distribution(q1_results[q1_results['challenge'] != 'none'])\n",
    "plot_sentiment_scores(q1_results[q1_results['challenge'] != 'none'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sentiment_distribution(q1_results[q1_results['continuity'] != 'none'], key = 'continuity', labels = continuity_responses)\n",
    "plot_sentiment_scores(q1_results[q1_results['continuity'] != 'none'], key = 'continuity', labels = continuity_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_sentiment_distribution(q1_results[q1_results['prepared'] != 'none'], key = 'prepared', labels = prepared_responses)\n",
    "plot_sentiment_scores(q1_results[q1_results['prepared'] != 'none'], key = 'prepared', labels = prepared_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_results['Length'] = merged_results['Original'].str.split().str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "q2_results = merged_results[merged_results['Question'] == Q2]\n",
    "plt.scatter(list(q2_results['continuity']), list(q2_results['Length']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_labels = {\n",
    "    'challenge': challenge_responses,\n",
    "    'continuity': continuity_responses,\n",
    "    'prepared': prepared_responses\n",
    "}\n",
    "\n",
    "sentiment_scores = {\n",
    "    'positive': 1,\n",
    "    'neutral': 0,\n",
    "    'negative': -1\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "q1_results['score'] = q1_results['prediction'].apply(lambda x: sentiment_scores[x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kruskal_wallis_sentiment(results, key = 'challenge'):\n",
    "    return kruskal(*[list(results[results[key] == label]['score']) for label in question_labels[key]], nan_policy = 'omit')\n",
    "\n",
    "def kruskal_wallis_response(results, key = 'challenge'):\n",
    "    response_scores = {question_labels[key][k]: k for k in range(len(question_labels[key]))}\n",
    "    results['response score'] = results[key].apply(lambda x: response_scores[x])\n",
    "    x = [list(results[results['prediction'] == sentiment]['response score']) for sentiment in SENTIMENTS]\n",
    "    return kruskal(*x, nan_policy = 'omit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def stat_summary(method):\n",
    "    print(method.__name__)\n",
    "    for key, value in question_labels.items():\n",
    "        print(f\"{key}: {method(q1_results, key = key)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(kruskal(*[list(q1_results[q1_results['Problem'] == problem]['score']) for problem in problems]),\n",
    "kruskal(*[list(q1_results[q1_results['Problem'] == problem]['score']) for problem in ['fex1', 'fex2', 'fex4']]),\n",
    "kruskal(*[list(q1_results[q1_results['Problem'] == problem]['score']) for problem in ['ps1', 'ps2', 'ps4']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_summary(kruskal_wallis_sentiment)\n",
    "stat_summary(kruskal_wallis_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is something I found at the article below, not sure if its relevant to what we're doing\n",
    "# https://en.wikipedia.org/wiki/Rank_correlation\n",
    "def spearmanr_sentiment_score(results, key = 'challenge'):\n",
    "    response_scores = {question_labels[key][k]: k for k in range(len(question_labels[key]))}\n",
    "    results['response score'] = results[key].apply(lambda x: response_scores[x])\n",
    "    return spearmanr(list(results['score']), list(results['response score']))\n",
    "\n",
    "def kendalltau_sentiment_score(results, key = 'challenge'):\n",
    "    response_scores = {question_labels[key][k]: k for k in range(len(question_labels[key]))}\n",
    "    results['response score'] = results[key].apply(lambda x: response_scores[x])\n",
    "    return kendalltau(list(results['score']), list(results['response score']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_summary(spearmanr_sentiment_score)\n",
    "stat_summary(kendalltau_sentiment_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_distribution(results, key = 'challenge'):\n",
    "    response_counts = []\n",
    "    for label in question_labels[key]:\n",
    "        response_count_label = []\n",
    "        for sentiment in SENTIMENTS:\n",
    "            response_count_label.append(len(results[(results['prediction'] == sentiment) & (results[key] == label)]))\n",
    "        response_counts.append(response_count_label)\n",
    "        \n",
    "    return pd.DataFrame(response_counts, columns = SENTIMENTS, index = question_labels[key])\n",
    "\n",
    "get_sentiment_distribution(q1_results, key = 'challenge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_responses_by_sentiment(results, key = 'challenge', normalize = False):\n",
    "    width = 0.35\n",
    "    sentiment_distribution = get_sentiment_distribution(results, key = key)\n",
    "    arr = np.array(sentiment_distribution)\n",
    "    if normalize:\n",
    "        arr = arr/np.sum(arr, axis = 0)\n",
    "    fig, ax = plt.subplots(figsize = (10, 10))\n",
    "    bars = []\n",
    "    for k in range(arr.shape[0]):\n",
    "        bars.append(ax.bar(SENTIMENTS, np.sum(arr[:arr.shape[0]-k], axis = 0)))\n",
    "    plt.legend(bars, sentiment_distribution.index[::-1])\n",
    "    return arr\n",
    "plot_responses_by_sentiment(q1_results, key = 'prepared', normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats._stats import _kendall_dis\n",
    "def SomersD(x, y):\n",
    "\n",
    "    x = np.asarray(x).ravel()\n",
    "    y = np.asarray(y).ravel()\n",
    "\n",
    "    if x.size != y.size:\n",
    "        raise ValueError(\"All inputs must be of the same size, \"\n",
    "                         \"found x-size %s and y-size %s\" % (x.size, y.size))\n",
    "\n",
    "    def count_rank_tie(ranks):\n",
    "        cnt = np.bincount(ranks).astype('int64', copy=False)\n",
    "        cnt = cnt[cnt > 1]\n",
    "        return ((cnt * (cnt - 1) // 2).sum(),\n",
    "            (cnt * (cnt - 1.) * (cnt - 2)).sum(),\n",
    "            (cnt * (cnt - 1.) * (2*cnt + 5)).sum())\n",
    "\n",
    "    size = x.size\n",
    "    perm = np.argsort(y)  # sort on y and convert y to dense ranks\n",
    "    x, y = x[perm], y[perm]\n",
    "    y = np.r_[True, y[1:] != y[:-1]].cumsum(dtype=np.intp)\n",
    "\n",
    "    # stable sort on x and convert x to dense ranks\n",
    "    perm = np.argsort(x, kind='mergesort')\n",
    "    x, y = x[perm], y[perm]\n",
    "    x = np.r_[True, x[1:] != x[:-1]].cumsum(dtype=np.intp)\n",
    "\n",
    "    dis = _kendall_dis(x, y)  # discordant pairs\n",
    "\n",
    "    obs = np.r_[True, (x[1:] != x[:-1]) | (y[1:] != y[:-1]), True]\n",
    "    cnt = np.diff(np.where(obs)[0]).astype('int64', copy=False)\n",
    "\n",
    "    ntie = (cnt * (cnt - 1) // 2).sum()  # joint ties\n",
    "    xtie, x0, x1 = count_rank_tie(x)     # ties in x, stats\n",
    "    ytie, y0, y1 = count_rank_tie(y)     # ties in y, stats\n",
    "\n",
    "    tot = (size * (size - 1)) // 2\n",
    "\n",
    "    # Note that tot = con + dis + (xtie - ntie) + (ytie - ntie) + ntie\n",
    "    #               = con + dis + xtie + ytie - ntie\n",
    "    #con_minus_dis = tot - xtie - ytie + ntie - 2 * dis\n",
    "    SD = (tot - xtie - ytie + ntie - 2 * dis) / (tot - ntie)\n",
    "    return (SD, dis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def somersd_sentiment_score(results, key = 'challenge'):\n",
    "    response_scores = {question_labels[key][k]: k for k in range(len(question_labels[key]))}\n",
    "    results['response score'] = results[key].apply(lambda x: response_scores[x])\n",
    "    return SomersD(list(results['score']), list(results['response score']))\n",
    "\n",
    "stat_summary(somersd_sentiment_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = list(complete_results['Question'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
