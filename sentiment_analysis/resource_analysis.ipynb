{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ac57dadc84aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSVC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpackages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0madd_newdocs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     __all__ = ['add_newdocs',\n\u001b[1;32m    144\u001b[0m                \u001b[0;34m'ModuleDeprecationWarning'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/numpy/add_newdocs.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdivision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0madd_newdoc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m###############################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/numpy/lib/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtype_check\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mindex_tricks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfunction_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/numpy/lib/type_check.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m            'common_type']\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumeric\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumeric\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0masarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misnan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzeros\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mufunclike\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0misneginf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misposinf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/numpy/core/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmultiarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     msg = \"\"\"\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.svm import SVC\n",
    "from scipy.stats import mannwhitneyu\n",
    "from scipy.stats import spearmanr\n",
    "import seaborn as sns\n",
    "\n",
    "from supervised_sentiment_analysis import *\n",
    "from constants import *\n",
    "from utilities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_results = pickle.load(open('merged_results.P', 'rb'))\n",
    "merged_results['Valid Vector'] = merged_results['Skip Thought Vector'].apply(lambda x: ~np.isnan(x).any())\n",
    "removed_results = merged_results[~merged_results['Valid Vector']]\n",
    "merged_results = merged_results[merged_results['Valid Vector']]\n",
    "labeled_results = merged_results[merged_results['Categorical Tag'] != 'no tag']\n",
    "q1_results = merged_results[merged_results['Question'] == Q1]\n",
    "q1_labeled_results = labeled_results[labeled_results['Question'] == Q1]\n",
    "q1_features = np.array(q1_labeled_results['Skip Thought Vector'].tolist())\n",
    "q1_labels = np.array(q1_labeled_results['Categorical Tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unprocessed_results = get_problem_data(DATA)\n",
    "complete_results = merge_problem_data(unprocessed_results)\n",
    "resource_results = complete_results[complete_results['Question'] == resource_question]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- get the resources associated with a student's response to question 1\n",
    "- this is the student's response to question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resources(row):\n",
    "    username = row['username']\n",
    "    problem = row['Problem']\n",
    "    resources = resource_results[(resource_results['username'] == username) & (resource_results['Problem'] == problem)]['Answer']\n",
    "    assert len(resources) < 2\n",
    "    if len(resources) == 0:\n",
    "        return []\n",
    "    resources = resources.iloc[0]\n",
    "    return resources.split(', ')\n",
    "q1_results['resources'] = q1_results.apply(get_resources, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- get the number of resources used by each student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_results['resources']\n",
    "def real_response_count(resource_list):\n",
    "    if 'please answer the next question.' in resource_list:\n",
    "        return len(resource_list)-1\n",
    "    return len(resource_list)\n",
    "    \n",
    "q1_results['resource count'] = q1_results['resources'].apply(real_response_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- get student sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC()\n",
    "clf.fit(q1_features, q1_labels)\n",
    "predictions = clf.predict(list(q1_results['Skip Thought Vector']))\n",
    "q1_results['sentiment'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_dict = {\n",
    "    'positive': 1,\n",
    "    'neutral': 0,\n",
    "    'negative': -1\n",
    "}\n",
    "q1_results['sentiment score'] = q1_results['sentiment'].apply(lambda x: sentiment_dict[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Calculate table 5 from the paper (see paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mannwhitney_resources(results, resource):\n",
    "    results['used resource'] = results['resources'].apply(lambda x: resource in x)\n",
    "    used = np.array(results[results['used resource']]['sentiment score'])\n",
    "    unused = np.array(results[~results['used resource']]['sentiment score'])\n",
    "    mwu = mannwhitneyu(used, unused, alternative = 'two-sided')\n",
    "    return [len(used), np.round(np.mean(used), 3), len(unused), np.round(np.mean(unused), 3), mwu[1]]\n",
    "\n",
    "mwu_stats = []\n",
    "for resource_answer in resource_answers:\n",
    "    mwu_stats.append(mannwhitney_resources(q1_results, resource_answer))\n",
    "    \n",
    "pd.DataFrame(np.array(mwu_stats), columns = ['used count', 'used sentiment', 'unused count', 'unused sentiment', 'Mann-Whitney U p-value'], index = resource_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- number of resources used by sentiment, and overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['positive', 'negative']\n",
    "fig, ax = plt.subplots(figsize = (8,8))\n",
    "sentiment_resource_counts = [list(q1_results[q1_results['sentiment'] == label]['resource count']) for label in classes]\n",
    "bins = range(10)\n",
    "arr = ax.hist(sentiment_resource_counts, bins, stacked=False, density=False, label = classes)\n",
    "ax.legend(prop={'size': 16})\n",
    "ax.set_xlabel('Number of resources used', size = 16)\n",
    "ax.set_xticks(range(10))\n",
    "ax.set_ylabel('Number of students', size = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,8))\n",
    "plt.hist(list(q1_results['resource count']), bins = range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = True\n",
    "\n",
    "def normalize_array(a, axis=-1, order=2):\n",
    "    l1 = np.atleast_1d(np.linalg.norm(a, order, axis))\n",
    "    l1[l1==0] = 1\n",
    "    return a / np.expand_dims(l1, axis)\n",
    "\n",
    "def get_problem_resource_counts(results, normalize = False):\n",
    "    problem_resource_counts = []\n",
    "    for problem in PROBLEMS:\n",
    "        problem_count = []\n",
    "        for resource in resource_answers:\n",
    "            resource_results['good'] = results.apply(lambda x: resource in x['Answer'].split(', ') and x['Problem'] == problem, axis = 1)\n",
    "            count = results[resource_results['good']].shape[0]/results[resource_results['Problem'] == problem].shape[0] if normalize else results[resource_results['good']].shape[0]\n",
    "            problem_count.append(count)\n",
    "        problem_resource_counts.append(problem_count)\n",
    "    return pd.DataFrame(np.array(problem_resource_counts), columns = resource_answers, index = PROBLEMS)\n",
    "\n",
    "problem_resource_df = get_problem_resource_counts(resource_results, normalize = n)\n",
    "problem_resource_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (12,8))\n",
    "fmt = '.2g' if n else 'd'\n",
    "sns.heatmap(problem_resource_df, cmap=plt.cm.Blues, ax = ax, square = True, annot = True, fmt = fmt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Various analyses comparing how challenging students found the course to be to the number of resources used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge_results = complete_results[complete_results['Question'] == challenge_question]\n",
    "def get_challenge(row):\n",
    "    username = row['username']\n",
    "    problem = row['Problem']\n",
    "    challenge_prediction = challenge_results[(challenge_results['username'] == username) & (challenge_results['Problem'] == problem)]['Answer']\n",
    "    assert len(challenge_prediction) < 2\n",
    "    if len(challenge_prediction) == 0:\n",
    "        return 'none'\n",
    "    return challenge_prediction.iloc[0]\n",
    "\n",
    "q1_results['challenge'] = q1_results.apply(get_challenge, axis = 1)\n",
    "q1_results['challenge score'] = q1_results['challenge'].apply(lambda x: challenge_responses.index(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spearmanr_challenge_resource_count(results):\n",
    "    return spearmanr(list(results['challenge score']), list(results['resource count']))\n",
    "spearmanr_challenge_resource_count(q1_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = True\n",
    "\n",
    "def get_challenge_resource_counts(results, normalize = False):\n",
    "    challenge_resource_counts = []\n",
    "    for challenge_response in challenge_responses:\n",
    "        challenge_count = []\n",
    "        for k in range(1, 9):\n",
    "            results['good'] = results.apply(lambda x: x['resource count'] == k and x['challenge'] == challenge_response, axis = 1)\n",
    "            count = results[results['good']].shape[0]/results[results['challenge'] == challenge_response].shape[0] if normalize else results[results['good']].shape[0]\n",
    "            challenge_count.append(count)\n",
    "        challenge_resource_counts.append(challenge_count)\n",
    "    return pd.DataFrame(np.array(challenge_resource_counts), columns = range(1, 9), index = challenge_responses)\n",
    "\n",
    "challenge_resource_df = get_challenge_resource_counts(q1_results, normalize = n)\n",
    "challenge_resource_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (12,6))\n",
    "fmt = '.2g' if n else 'd'\n",
    "sns.heatmap(challenge_resource_df, cmap=plt.cm.Blues, ax = ax, square = True, annot = True, fmt = fmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
